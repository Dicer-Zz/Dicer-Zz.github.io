<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>sklearn踩坑 - Dicer&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Dicer&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Dicer&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="这几天在做毕业设计，想做一个微博的情感分析，想着实现两种方式，一是朴素贝叶斯，二是用LSTM。 在做朴素贝叶斯的时候，据网上看到的一些文章说，训练的速度应该是很快的。但是我的训练速度却很慢，分析了一下发现是文本分词、清洗占去了大量的时间。我的语料大概12w行，20MB左右。文本处理需要一分多钟，而朴素贝叶斯的训练时间只需要一秒钟左右。 于是想把文本处理的结果，保存起来，下次直接使用，就不需要每次都"><meta property="og:type" content="blog"><meta property="og:title" content="Dicer&#039;s Blog"><meta property="og:url" content="https://blog.dicer.fun/2021/05/05/sklearn%E8%B8%A9%E5%9D%91/"><meta property="og:site_name" content="Dicer&#039;s blog"><meta property="og:description" content="这几天在做毕业设计，想做一个微博的情感分析，想着实现两种方式，一是朴素贝叶斯，二是用LSTM。 在做朴素贝叶斯的时候，据网上看到的一些文章说，训练的速度应该是很快的。但是我的训练速度却很慢，分析了一下发现是文本分词、清洗占去了大量的时间。我的语料大概12w行，20MB左右。文本处理需要一分多钟，而朴素贝叶斯的训练时间只需要一秒钟左右。 于是想把文本处理的结果，保存起来，下次直接使用，就不需要每次都"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://blog.dicer.fun/gallery/cover/sklearn.png"><meta property="article:published_time" content="2021-05-05T07:37:56.000Z"><meta property="article:modified_time" content="2021-05-05T08:11:03.899Z"><meta property="article:author" content="Dicer"><meta property="article:tag" content="sklearn"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/gallery/cover/sklearn.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.dicer.fun/2021/05/05/sklearn%E8%B8%A9%E5%9D%91/"},"headline":"sklearn踩坑","image":["https://blog.dicer.fun/gallery/cover/sklearn.png"],"datePublished":"2021-05-05T07:37:56.000Z","dateModified":"2021-05-05T08:11:03.899Z","author":{"@type":"Person","name":"Dicer"},"description":"这几天在做毕业设计，想做一个微博的情感分析，想着实现两种方式，一是朴素贝叶斯，二是用LSTM。 在做朴素贝叶斯的时候，据网上看到的一些文章说，训练的速度应该是很快的。但是我的训练速度却很慢，分析了一下发现是文本分词、清洗占去了大量的时间。我的语料大概12w行，20MB左右。文本处理需要一分多钟，而朴素贝叶斯的训练时间只需要一秒钟左右。 于是想把文本处理的结果，保存起来，下次直接使用，就不需要每次都"}</script><link rel="canonical" href="https://blog.dicer.fun/2021/05/05/sklearn%E8%B8%A9%E5%9D%91/"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/favicon.png" alt="Dicer&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Dicer-Zz"><i class="fab fa-github"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Home Page" href="https://dicer.fun"><i class="fas fa-dice"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/gallery/cover/sklearn.png" alt="sklearn踩坑"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-05-05T07:37:56.000Z" title="2021/5/5 下午3:37:56">2021-05-05</time>发表</span><span class="level-item"><time dateTime="2021-05-05T08:11:03.899Z" title="2021/5/5 下午4:11:03">2021-05-05</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/NLP/">NLP</a></span><span class="level-item">8 分钟读完 (大约1204个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">sklearn踩坑</h1><div class="content"><p>这几天在做毕业设计，想做一个微博的情感分析，想着实现两种方式，一是朴素贝叶斯，二是用LSTM。</p>
<p>在做朴素贝叶斯的时候，据网上看到的一些文章说，训练的速度应该是很快的。但是我的训练速度却很慢，分析了一下发现是文本分词、清洗占去了大量的时间。我的语料大概12w行，20MB左右。文本处理需要一分多钟，而朴素贝叶斯的训练时间只需要一秒钟左右。</p>
<p>于是想把文本处理的结果，保存起来，下次直接使用，就不需要每次都多等一分钟了。</p>
<span id="more"></span>

<h1 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h1><p>原始文本的格式（csv格式）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">label,review</span><br><span class="line">1,更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]</span><br><span class="line">1,@张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心，酒店都全部OK啦。</span><br><span class="line">1,姑娘都羡慕你呢…还有招财猫高兴……&#x2F;&#x2F;@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢&#x2F;&#x2F;@李欣芸SharonLee:大佬范儿[书呆子]</span><br><span class="line">1,美~~~~~[爱你]</span><br></pre></td></tr></table></figure>

<p>处理后的文件（csv格式）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">label,review</span><br><span class="line">1,更博 爆照 帅 的 呀 越来越 爱 生快 傻 缺 [爱你] [爱你] [爱你]</span><br><span class="line">1,土耳其 的 事要 认真对待 [哈哈] 直接 开除 是 细心 酒店 都 全部</span><br><span class="line">1,姑娘 都 羡慕 呢 招财猫 高兴 [哈哈] 小 学徒 一枚 等 着 明天 见 呢 大佬 范儿 [书呆子]</span><br><span class="line">1,美 [爱你]</span><br></pre></td></tr></table></figure>

<p>处理文件的脚本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trim</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    带有语料清洗功能的分词函数, 包含数据预处理, 可以根据自己的需求重载</span></span><br><span class="line"><span class="string">    使用re保证了一些本来可能会分开的表情图标不分开</span></span><br><span class="line"><span class="string">    return: [str]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    text = re.sub(<span class="string">&quot;\&#123;%.+?%\&#125;&quot;</span>, <span class="string">&quot; &quot;</span>, text)           <span class="comment"># 去除 &#123;%xxx%&#125; (地理定位, 微博话题等)</span></span><br><span class="line">    <span class="comment"># text = re.sub(&quot;@.+?( |$)&quot;, &quot; &quot;, text)           # 去除 @xxx (用户名)</span></span><br><span class="line">    text = re.sub(<span class="string">&quot;@.+?( |:)&quot;</span>, <span class="string">&quot; &quot;</span>, text)           <span class="comment"># 去除 @xxx (用户名)</span></span><br><span class="line">    text = re.sub(<span class="string">&quot;【.+?】&quot;</span>, <span class="string">&quot; &quot;</span>, text)              <span class="comment"># 去除 【xx】 (里面的内容通常都不是用户自己写的)</span></span><br><span class="line">    text = re.sub(<span class="string">&quot;[a-zA-Z0-9]&quot;</span>, <span class="string">&quot; &quot;</span>, text)         <span class="comment"># 去除字母和数字</span></span><br><span class="line">    icons = re.findall(<span class="string">&quot;\[.+?\]&quot;</span>, text)             <span class="comment"># 提取出所有表情图标</span></span><br><span class="line">    text = re.sub(<span class="string">&quot;\[.+?\]&quot;</span>, <span class="string">&quot;IconMark&quot;</span>, text)      <span class="comment"># 将文本中的图标替换为`IconMark`</span></span><br><span class="line"></span><br><span class="line">    tokens = []</span><br><span class="line">    <span class="comment"># for k, w in enumerate(jieba.lcut(text)):</span></span><br><span class="line">    jieba.load_userdict(<span class="string">&#x27;./data/user_dict.txt&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> jieba.cut(text):</span><br><span class="line">        w = w.strip()</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;IconMark&quot;</span> <span class="keyword">in</span> w:                         <span class="comment"># 将IconMark替换为原图标</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(w.count(<span class="string">&quot;IconMark&quot;</span>)):</span><br><span class="line">                tokens.append(icons.pop(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">elif</span> w <span class="keyword">and</span> w != <span class="string">&#x27;\u200b&#x27;</span> <span class="keyword">and</span> w.isalpha():   <span class="comment"># 只保留有效文本</span></span><br><span class="line">            tokens.append(w)</span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_corpus</span>(<span class="params">csvFilePath, stopwordPath</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加载语料库，并进行分词，数据清洗，去除停用词</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 数据读取</span></span><br><span class="line">    df = pd.read_csv(csvFilePath)</span><br><span class="line">    stopword = load_stopword(stopwordPath)</span><br><span class="line">    labels, reviews = df[<span class="string">&#x27;label&#x27;</span>].to_list(), df[<span class="string">&#x27;review&#x27;</span>].to_list()</span><br><span class="line">    trimedReviews = []</span><br><span class="line">    <span class="keyword">for</span> review <span class="keyword">in</span> reviews:</span><br><span class="line">        <span class="comment"># 数据清洗</span></span><br><span class="line">        trimedReview = trim(review)</span><br><span class="line">        <span class="comment"># 去除停用词</span></span><br><span class="line">        finalReview = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> trimedReview:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopword:</span><br><span class="line">                finalReview.append(word)</span><br><span class="line">        trimedReviews.append(finalReview)</span><br><span class="line">    <span class="keyword">return</span> labels, trimedReviews</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_reviews</span>(<span class="params">csvFilePath</span>):</span></span><br><span class="line">    df = pd.read_csv(csvFilePath)</span><br><span class="line">    <span class="keyword">return</span> df[<span class="string">&#x27;label&#x27;</span>], df[<span class="string">&#x27;review&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_stopword</span>(<span class="params">filePath</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加载停用词</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filePath, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> words:</span><br><span class="line">        stopword = [word.strip() <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">    <span class="keyword">return</span> stopword</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_suffle</span>(<span class="params">labels, reviews</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    打乱数据</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    join = <span class="built_in">list</span>(<span class="built_in">zip</span>(labels, reviews))</span><br><span class="line">    random.shuffle(join)</span><br><span class="line">    labels, reviews = <span class="built_in">zip</span>(*join)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(labels), <span class="built_in">list</span>(reviews)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pre_trim</span>(<span class="params">csvFilePath, stopwordPath</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    预处理csv文本，并持久化</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    df = pd.read_csv(csvFilePath)</span><br><span class="line">    _, reviews = load_corpus(csvFilePath, stopwordPath)</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(reviews)):</span><br><span class="line">        reviews[index] = <span class="string">&#x27; &#x27;</span>.join(reviews[index])</span><br><span class="line">    df[<span class="string">&#x27;review&#x27;</span>] = reviews</span><br><span class="line">    df.to_csv(csvFilePath[:-<span class="number">4</span>] + <span class="string">&#x27;Trimed.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    csvFilePath = <span class="string">&#x27;../../corpus/100k/all.csv&#x27;</span></span><br><span class="line">    stopwordPath = <span class="string">&#x27;./data/stopword.txt&#x27;</span></span><br><span class="line">    pre_trim(csvFilePath, stopwordPath)</span><br></pre></td></tr></table></figure>

<h1 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h1><h2 id="读取文件并分割数据集"><a href="#读取文件并分割数据集" class="headerlink" title="读取文件并分割数据集"></a>读取文件并分割数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer, TfidfTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_reviews, data_suffle</span><br><span class="line"></span><br><span class="line"><span class="comment"># stopwordPath = &#x27;./data/stopword.txt&#x27;</span></span><br><span class="line"><span class="comment"># userDictPath = &#x27;./data/user_dict.txt&#x27;</span></span><br><span class="line">csvFilePath = <span class="string">&#x27;../../corpus/100k/allTrimed.csv&#x27;</span></span><br><span class="line">modelPath = <span class="string">&#x27;./data/bayes.model&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入自定义字典</span></span><br><span class="line"><span class="comment"># jieba.load_userdict(userDictPath)</span></span><br><span class="line"></span><br><span class="line">time_start = time.time()</span><br><span class="line"></span><br><span class="line">labels, reviews = load_reviews(csvFilePath)</span><br><span class="line">labels, reviews = data_suffle(labels, reviews)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1/4 分割数据集</span></span><br><span class="line">n = <span class="built_in">len</span>(labels) // <span class="number">5</span></span><br><span class="line">labels_train, reviews_train = labels[n:], reviews[n:]</span><br><span class="line">labels_test, reviews_test = labels[:n], reviews[:n]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Load Corpus Cost <span class="subst">&#123;time.time() - time_start:<span class="number">.4</span>f&#125;</span> Sec&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">time_start = time.time()</span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer(max_df=<span class="number">0.8</span>, min_df=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># ⚠️坑点</span></span><br><span class="line">vec_train = vectorizer.fit_transform([np.str_(review) <span class="keyword">for</span> review <span class="keyword">in</span> reviews_train])</span><br><span class="line">clf = MultinomialNB().fit(vec_train, labels_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Train Model Cost <span class="subst">&#123;time.time() - time_start:<span class="number">.4</span>f&#125;</span> Sec&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>这个地方就是我遇到的问题，如果不将<code>reviews</code>全部转为<code>np.string</code>的话，会报<code>ValueError: np.nan is an invalid document, expected byte or unicode string.</code>错误，但是根据sklearn的文档，似乎并没有说不能传入<code>str</code>类型。</p>
<h2 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vec_test = vectorizer.transform([np.str_(review) <span class="keyword">for</span> review <span class="keyword">in</span> reviews_test])</span><br><span class="line">pred = clf.predict(vec_test)</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="built_in">print</span>(metrics.classification_report(labels_test, pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准确率:&quot;</span>, metrics.accuracy_score(labels_test, pred))</span><br></pre></td></tr></table></figure>

<p>预测的准确率还不错，能达到85%以上。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>sklearn踩坑</p><p><a href="https://blog.dicer.fun/2021/05/05/sklearn踩坑/">https://blog.dicer.fun/2021/05/05/sklearn踩坑/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Dicer</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-05-05</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-05-05</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/sklearn/">sklearn</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/alipayQR.JPG" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wechatQR.JPG" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/05/18/%E5%9F%BA%E4%BA%8Eword2vec%E7%9A%84%E7%BA%A2%E6%A5%BC%E6%A2%A6%E4%BA%BA%E7%89%A9%E5%85%B3%E7%B3%BB%E5%88%86%E6%9E%90/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">基于word2vec的红楼梦人物关系分析</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/04/25/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"><span class="level-item">博客迁移</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="Dicer"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Dicer</p><p class="is-size-6 is-block">Trying to reach the star.</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">34</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">37</p></a></div></div></nav></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><figure class="media-left"><a class="image" href="/2021/05/18/%E5%9F%BA%E4%BA%8Eword2vec%E7%9A%84%E7%BA%A2%E6%A5%BC%E6%A2%A6%E4%BA%BA%E7%89%A9%E5%85%B3%E7%B3%BB%E5%88%86%E6%9E%90/"><img src="/gallery/cover/hlm.png" alt="基于word2vec的红楼梦人物关系分析"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-05-18T09:02:44.000Z">2021-05-18</time></p><p class="title"><a href="/2021/05/18/%E5%9F%BA%E4%BA%8Eword2vec%E7%9A%84%E7%BA%A2%E6%A5%BC%E6%A2%A6%E4%BA%BA%E7%89%A9%E5%85%B3%E7%B3%BB%E5%88%86%E6%9E%90/">基于word2vec的红楼梦人物关系分析</a></p><p class="categories"><a href="/categories/NLP/">NLP</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/05/05/sklearn%E8%B8%A9%E5%9D%91/"><img src="/gallery/cover/sklearn.png" alt="sklearn踩坑"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-05-05T07:37:56.000Z">2021-05-05</time></p><p class="title"><a href="/2021/05/05/sklearn%E8%B8%A9%E5%9D%91/">sklearn踩坑</a></p><p class="categories"><a href="/categories/NLP/">NLP</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/04/25/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"><img src="/gallery/cover/gitpage.png" alt="博客迁移"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-04-25T13:39:02.000Z">2021-04-25</time></p><p class="title"><a href="/2021/04/25/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/">博客迁移</a></p><p class="categories"><a href="/categories/%E9%9A%8F%E7%AC%94/">随笔</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/04/12/Word2vec-example/"><img src="/gallery/thumbnails/dicer.png" alt="word2vec实例"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-04-12T07:33:50.000Z">2021-04-12</time></p><p class="title"><a href="/2021/04/12/Word2vec-example/">word2vec实例</a></p><p class="categories"><a href="/categories/NLP/">NLP</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/03/14/Nginx-%E6%B7%BB%E5%8A%A0SSL%E8%AF%81%E4%B9%A6/"><img src="/gallery/thumbnails/SSL.png" alt="Nginx 添加SSL证书"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-03-14T13:51:47.000Z">2021-03-14</time></p><p class="title"><a href="/2021/03/14/Nginx-%E6%B7%BB%E5%8A%A0SSL%E8%AF%81%E4%B9%A6/">Nginx 添加SSL证书</a></p><p class="categories"><a href="/categories/%E7%BD%91%E7%BB%9C/">网络</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/favicon.png" alt="Dicer&#039;s blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Dicer</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Dicer-Zz"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: false,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>